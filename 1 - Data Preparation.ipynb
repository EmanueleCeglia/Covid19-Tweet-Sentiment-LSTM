{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import tweepy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 9885107: expected 12 fields, saw 13\\n'\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('TweetsCOV19_3.tsv', sep='\\t', error_bad_lines=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns\n",
    "dataframe.columns = ['id','username','date','followers','friends','retweets','favorites','entities','sentiment','mentions','hashtags','url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all rows with nan and 'null;' values in column hashtags and entities\n",
    "df = dataframe[dataframe['hashtags'].notna()]\n",
    "df = df.loc[df['hashtags']!='null;',]\n",
    "df = df.loc[df['entities']!='null;',]\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New dataframe that will be filled\n",
    "df_clean = pd.DataFrame(columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index of the rows which satisfy the hashtag requirements\n",
    "index = np.array([])\n",
    "for i in range(len(df)):\n",
    "    if ('covid19' in df['hashtags'][i]) or ('coronavirus' in df['hashtags'][i]) or ('covid' in df['hashtags'][i]) or ('covaccine' in df['hashtags'][i]) or ('lockdown' in df['hashtags'][i]) or ('homequarantine' in df['hashtags'][i]) or ('quarantinecenter' in df['hashtags'][i]) or ('socialdistancing' in df['hashtags'][i]) or ('stayhome' in df['hashtags'][i]) or ('staysafe' in df['hashtags'][i]):\n",
    "        index = np.append(index, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill the New dataframe with rows == index\n",
    "df_clean = df[df.index.isin(index)]\n",
    "df_clean = df_clean.reset_index()\n",
    "#Drop the column 'username'\n",
    "df_clean = df_clean.drop('username', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe as a CSV to improve the efficiency \n",
    "df_clean.to_csv('df3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKPOINT - Load the dataframe\n",
    "df = pd.read_csv('df3.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TWITTER API in order to download the text of every tweet\n",
    "consumer_key = \"uSxZPLHGWcGBE9TCeawvoK0TN\"\n",
    "consumer_secret = \"YBLPpGGJ2zPDKyvFaY1Kz9aEYBToINZpaOAjOhB8ZrgxEigUhY\"\n",
    "access_token = \"1481296108492046337-EWsoOM8A8YSO1X0uFk0EbuIIMMEcW9\"\n",
    "access_token_secret = \"x5UZQLD24klamFOWliTTJXp9P3L8lTbHomBWk09q9VSCg\"\n",
    "\n",
    "# authorization of consumer key and consumer secret\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "  \n",
    "# set access to user's access key and access secret \n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "  \n",
    "# calling the api \n",
    "api = tweepy.API(auth)\n",
    "\n",
    "lista_tweets = {}\n",
    "# list of status IDs to be fetched \n",
    "for i in range(0,len(df),100):\n",
    "    id_ = list(df['id'].values)[i:i+100]\n",
    "    # fetching the statuses\n",
    "    statuses = api.lookup_statuses(id_, tweet_mode='extended')\n",
    "    \n",
    "    for status in statuses:\n",
    "        lista_tweets[status.id_str] = status.full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147941/147941 [40:10<00:00, 61.36it/s] \n"
     ]
    }
   ],
   "source": [
    "#Remove all rows whitout text\n",
    "c = 0\n",
    "for i in tqdm(df['id']):\n",
    "    if (str(i) in lista_tweets.keys()) == False:\n",
    "        df = df.drop(c,axis=0)\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('level_0', axis=1)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Order the dictionary likes the dataframe\n",
    "sorted_dict = OrderedDict([(el, lista_tweets[str(el)]) for el in list(df['id'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column with all texts\n",
    "df['Text'] = list(sorted_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe in order to improve the efficiency\n",
    "df.to_csv('df3_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKPOINT\n",
    "df = pd.read_csv('df3_text.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47726/47726 [00:18<00:00, 2571.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#Cleaner\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "clean_text = []\n",
    "for or_text in tqdm(df['Text']):\n",
    "    text = or_text\n",
    "    \n",
    "    #Remove links from the corpus of every tweet\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    \n",
    "    #Remove emoticons from the corpus of every tweet\n",
    "    text = remove_emoji(text)\n",
    "    \n",
    "    #Remove hashtags from the corpus of every tweet\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if (('#' in word) == False) and (('@' in word) == False):\n",
    "            new_text.append(word)\n",
    "    text = ' '.join(new_text)\n",
    "    \n",
    "    #Remove stop words from the corpus of every tweet\n",
    "    sw_nltk = stopwords.words('english')\n",
    "    words = [word for word in text.split() if word.lower() not in sw_nltk]\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    #Remove punctuation from the corpus of every tweet\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?!@’#+$%^&*_~“”'''\n",
    "    for ele in text:\n",
    "        if ele in punc:\n",
    "            text = text.replace(ele, \"\")\n",
    "    \n",
    "    #Remove some words from the corpus of every tweet\n",
    "    no_words = ['null','https','don','i','it','My','to','and','amp','s','t','don','m','us','can','not','l','2','we','re','you','I','ve','“','”','r','it','|','•','I']\n",
    "    for word in text.split():\n",
    "        if word in no_words:\n",
    "            text = text.replace(word, '')\n",
    "    \n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    clean_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the final df cleaned\n",
    "df.to_csv('df3_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
